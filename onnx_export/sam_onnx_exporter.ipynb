{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed10dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Any\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import trunc_normal_\n",
    "\n",
    "\n",
    "from sam2.modeling.sam2_base import SAM2Base\n",
    "\n",
    "class SAM2ImageEncoder(nn.Module):\n",
    "    def __init__(self, sam_model: SAM2Base) -> None:\n",
    "        super().__init__()\n",
    "        self.model = sam_model\n",
    "        self.image_encoder = sam_model.image_encoder\n",
    "        self.no_mem_embed = sam_model.no_mem_embed\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[Any, Any, Any]:\n",
    "        backbone_out = self.image_encoder(x)\n",
    "        backbone_out[\"backbone_fpn\"][0] = self.model.sam_mask_decoder.conv_s0(\n",
    "            backbone_out[\"backbone_fpn\"][0]\n",
    "        )\n",
    "        backbone_out[\"backbone_fpn\"][1] = self.model.sam_mask_decoder.conv_s1(\n",
    "            backbone_out[\"backbone_fpn\"][1]\n",
    "        )\n",
    "\n",
    "        feature_maps = backbone_out[\"backbone_fpn\"][-self.model.num_feature_levels:]\n",
    "        vision_pos_embeds = backbone_out[\"vision_pos_enc\"][-self.model.num_feature_levels:]\n",
    "\n",
    "        feat_sizes = [(x.shape[-2], x.shape[-1]) for x in vision_pos_embeds]\n",
    "\n",
    "        # flatten NxCxHxW to HWxNxC\n",
    "        vision_feats = [x.flatten(2).permute(2, 0, 1) for x in feature_maps]\n",
    "        vision_pos_embeds = [x.flatten(2).permute(2, 0, 1) for x in vision_pos_embeds]\n",
    "\n",
    "        vision_feats[-1] = vision_feats[-1] + self.no_mem_embed\n",
    "\n",
    "        feats = [feat.permute(1, 2, 0).reshape(1, -1, *feat_size)\n",
    "                 for feat, feat_size in zip(vision_feats[::-1], feat_sizes[::-1])][::-1]\n",
    "\n",
    "        return feats[0], feats[1], feats[2]\n",
    "\n",
    "\n",
    "class SAM2ImageDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            sam_model: SAM2Base,\n",
    "            multimask_output: bool\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mask_decoder = sam_model.sam_mask_decoder\n",
    "        self.prompt_encoder = sam_model.sam_prompt_encoder\n",
    "        self.model = sam_model\n",
    "        self.multimask_output = multimask_output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "            self,\n",
    "            image_embed: torch.Tensor,\n",
    "            high_res_feats_0: torch.Tensor,\n",
    "            high_res_feats_1: torch.Tensor,\n",
    "            point_coords: torch.Tensor,\n",
    "            point_labels: torch.Tensor\n",
    "    ):\n",
    "        sparse_embedding = self._embed_points(point_coords, point_labels)\n",
    "        self.sparse_embedding = sparse_embedding\n",
    "        dense_embedding = self.prompt_encoder.no_mask_embed.weight.reshape(1, -1, 1, 1)\n",
    "\n",
    "        high_res_feats = [high_res_feats_0, high_res_feats_1]\n",
    "        image_embed = image_embed\n",
    "\n",
    "        masks, iou_predictions, _, _ = self.mask_decoder.predict_masks(\n",
    "            image_embeddings=image_embed,\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embedding,\n",
    "            dense_prompt_embeddings=dense_embedding,\n",
    "            repeat_image=False,\n",
    "            high_res_features=high_res_feats,\n",
    "        )\n",
    "\n",
    "        if self.multimask_output:\n",
    "            masks = masks[:, 1:, :, :]\n",
    "            iou_predictions = iou_predictions[:, 1:]\n",
    "        else:\n",
    "            masks, iou_predictions = self.mask_decoder._dynamic_multimask_via_stability(masks, iou_predictions)\n",
    "\n",
    "        masks = torch.clamp(masks, -32.0, 32.0)\n",
    "        masks = masks > 0.0\n",
    "        masks = masks.to(torch.float32)\n",
    "        masks = masks * 255.0\n",
    "\n",
    "        return masks, iou_predictions\n",
    "\n",
    "    def _embed_points(self, point_coords: torch.Tensor, point_labels: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        point_coords = point_coords + 0.5\n",
    "\n",
    "        padding_point = torch.zeros((point_coords.shape[0], 1, 2), device=point_coords.device)\n",
    "        padding_label = -torch.ones((point_labels.shape[0], 1), device=point_labels.device)\n",
    "        point_coords = torch.cat([point_coords, padding_point], dim=1)\n",
    "        point_labels = torch.cat([point_labels, padding_label], dim=1)\n",
    "\n",
    "        point_coords[:, :, 0] = point_coords[:, :, 0] / self.model.image_size\n",
    "        point_coords[:, :, 1] = point_coords[:, :, 1] / self.model.image_size\n",
    "\n",
    "        point_embedding = self.prompt_encoder.pe_layer._pe_encoding(point_coords)\n",
    "        point_labels = point_labels.unsqueeze(-1).expand_as(point_embedding)\n",
    "\n",
    "        point_embedding = point_embedding * (point_labels != -1)\n",
    "        point_embedding = point_embedding + self.prompt_encoder.not_a_point_embed.weight * (\n",
    "                point_labels == -1\n",
    "        )\n",
    "\n",
    "        for i in range(self.prompt_encoder.num_point_embeddings):\n",
    "            point_embedding = point_embedding + self.prompt_encoder.point_embeddings[i].weight * (point_labels == i)\n",
    "\n",
    "        return point_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a2c097-8ee5-4065-8b75-36dbb088aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'sam2.1_hiera_small' #@param [\"sam2_hiera_tiny\", \"sam2_hiera_small\", \"sam2_hiera_large\", \"sam2_hiera_base_plus\"]\n",
    "# input_size = 768 #@param {type:\"slider\", min:160, max:4102, step:8}\n",
    "input_size = 1024 # Bad output if anything else (for now)\n",
    "multimask_output = False\n",
    "\n",
    "if model_type == \"sam2.1_hiera_tiny\":\n",
    "    model_cfg = \"configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "elif model_type == \"sam2.1_hiera_small\":\n",
    "    model_cfg = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
    "elif model_type == \"sam2.1_hiera_base_plus\":\n",
    "    model_cfg = \"configs/sam2.1/sam2.1_hiera_b+.yaml\"\n",
    "elif model_type == \"sam2.1_hiera_large\":\n",
    "    model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd824e40-aeeb-4c89-975d-6ad502752de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 256, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Datasets_and_code\\code\\realsense_python\\render_project\\final2\\python\\onnx_exporter\\sam2\\sam2\\modeling\\backbones\\utils.py:30: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_h > 0 or pad_w > 0:\n",
      "D:\\Datasets_and_code\\code\\realsense_python\\render_project\\final2\\python\\onnx_exporter\\sam2\\sam2\\modeling\\backbones\\utils.py:58: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if Hp > H or Wp > W:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "\n",
    "sam2_checkpoint = f\"checkpoints/{model_type}.pt\"\n",
    "\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cpu\")\n",
    "\n",
    "img=torch.randn(1, 3, input_size, input_size).cpu()\n",
    "\n",
    "sam2_encoder = SAM2ImageEncoder(sam2_model).cpu()\n",
    "high_res_feats_0, high_res_feats_1, image_embed = sam2_encoder(img)\n",
    "print(high_res_feats_0.shape)\n",
    "print(high_res_feats_1.shape)\n",
    "print(image_embed.shape)\n",
    "\n",
    "torch.onnx.export(sam2_encoder,\n",
    "                  img,\n",
    "                  f\"{model_type}_encoder.onnx\",\n",
    "                  export_params=True,\n",
    "                  opset_version=16,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names = ['image'],\n",
    "                  output_names = ['high_res_feats_0', 'high_res_feats_1', 'image_embed']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e542e6a-36fd-46ba-a7b2-a5de56503fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sam2_decoder = SAM2ImageDecoder(sam2_model, multimask_output=multimask_output).cpu()\\n\\nembed_dim = sam2_model.sam_prompt_encoder.embed_dim\\nembed_size = (sam2_model.image_size // sam2_model.backbone_stride, sam2_model.image_size // sam2_model.backbone_stride)\\nmask_input_size = [4 * x for x in embed_size]\\nprint(embed_dim, embed_size, mask_input_size)\\n\\npoint_coords = torch.randint(low=0, high=input_size, size=(1, 5, 2), dtype=torch.float)\\npoint_labels = torch.randint(low=0, high=1, size=(1, 5), dtype=torch.float)\\nmask_input = torch.randn(1, 1, *mask_input_size, dtype=torch.float)\\nhas_mask_input = torch.tensor([1], dtype=torch.float)\\norig_im_size = torch.tensor([input_size, input_size], dtype=torch.int32)\\n\\nmasks, scores = sam2_decoder(image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size)\\n\\n\\ntorch.onnx.export(sam2_decoder,\\n                  (image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size),\\n                  f\"{model_type}_decoder.onnx\",\\n                  export_params=True,\\n                  opset_version=16,\\n                  do_constant_folding=True,\\n                  input_names = [\\'image_embed\\', \\'high_res_feats_0\\', \\'high_res_feats_1\\', \\'point_coords\\', \\'point_labels\\', \\'mask_input\\', \\'has_mask_input\\', \\'orig_im_size\\'],\\n                  output_names = [\\'masks\\', \\'iou_predictions\\'],\\n                  dynamic_axes = {\"point_coords\": {0: \"num_labels\", 1: \"num_points\"},\\n                                  \"point_labels\": {0: \"num_labels\", 1: \"num_points\"},\\n                                  \"mask_input\": {0: \"num_labels\"},\\n                                  \"has_mask_input\": {0: \"num_labels\"}\\n                  }\\n                )'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sam2_decoder = SAM2ImageDecoder(sam2_model, multimask_output=multimask_output).cpu()\n",
    "\n",
    "embed_dim = sam2_model.sam_prompt_encoder.embed_dim\n",
    "embed_size = (sam2_model.image_size // sam2_model.backbone_stride, sam2_model.image_size // sam2_model.backbone_stride)\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "print(embed_dim, embed_size, mask_input_size)\n",
    "\n",
    "point_coords = torch.randint(low=0, high=input_size, size=(1, 5, 2), dtype=torch.float)\n",
    "point_labels = torch.randint(low=0, high=1, size=(1, 5), dtype=torch.float)\n",
    "mask_input = torch.randn(1, 1, *mask_input_size, dtype=torch.float)\n",
    "has_mask_input = torch.tensor([1], dtype=torch.float)\n",
    "orig_im_size = torch.tensor([input_size, input_size], dtype=torch.int32)\n",
    "\n",
    "masks, scores = sam2_decoder(image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size)\n",
    "\n",
    "\n",
    "torch.onnx.export(sam2_decoder,\n",
    "                  (image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size),\n",
    "                  f\"{model_type}_decoder.onnx\",\n",
    "                  export_params=True,\n",
    "                  opset_version=16,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names = ['image_embed', 'high_res_feats_0', 'high_res_feats_1', 'point_coords', 'point_labels', 'mask_input', 'has_mask_input', 'orig_im_size'],\n",
    "                  output_names = ['masks', 'iou_predictions'],\n",
    "                  dynamic_axes = {\"point_coords\": {0: \"num_labels\", 1: \"num_points\"},\n",
    "                                  \"point_labels\": {0: \"num_labels\", 1: \"num_points\"},\n",
    "                                  \"mask_input\": {0: \"num_labels\"},\n",
    "                                  \"has_mask_input\": {0: \"num_labels\"}\n",
    "                  }\n",
    "                )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c68770-933c-42f9-8a28-fb402afe61bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sam2_decoder = SAM2ImageDecoder(sam2_model, multimask_output=multimask_output).cpu()\\n\\nembed_dim = sam2_model.sam_prompt_encoder.embed_dim\\nembed_size = (sam2_model.image_size // sam2_model.backbone_stride, sam2_model.image_size // sam2_model.backbone_stride)\\nmask_input_size = [4 * x for x in embed_size]\\nprint(embed_dim, embed_size, mask_input_size)\\n\\npoint_coords = torch.randint(low=0, high=input_size, size=(1, 5, 2), dtype=torch.float)\\npoint_labels = torch.randint(low=0, high=1, size=(1, 5), dtype=torch.float)\\n#mask_input = torch.randn(1, 1, *mask_input_size, dtype=torch.float)\\n#has_mask_input = torch.tensor([1], dtype=torch.float)\\n\\nmasks, scores = sam2_decoder(image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels)\\n\\n\\ntorch.onnx.export(sam2_decoder,\\n                  (image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels),\\n                  f\"{model_type}_decoder.onnx\",\\n                  export_params=True,\\n                  opset_version=16,\\n                  do_constant_folding=True,\\n                  input_names = [\\'image_embed\\', \\'high_res_feats_0\\', \\'high_res_feats_1\\', \\'point_coords\\', \\'point_labels\\'],\\n                  output_names = [\\'masks\\', \\'iou_predictions\\'],\\n                  #dynamic_axes = {\"point_coords\": {1: \"num_points\"},\\n                                  #\"point_labels\": {1: \"num_points\"},\\n                  #}\\n                )'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sam2_decoder = SAM2ImageDecoder(sam2_model, multimask_output=multimask_output).cpu()\n",
    "\n",
    "embed_dim = sam2_model.sam_prompt_encoder.embed_dim\n",
    "embed_size = (sam2_model.image_size // sam2_model.backbone_stride, sam2_model.image_size // sam2_model.backbone_stride)\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "print(embed_dim, embed_size, mask_input_size)\n",
    "\n",
    "point_coords = torch.randint(low=0, high=input_size, size=(1, 5, 2), dtype=torch.float)\n",
    "point_labels = torch.randint(low=0, high=1, size=(1, 5), dtype=torch.float)\n",
    "#mask_input = torch.randn(1, 1, *mask_input_size, dtype=torch.float)\n",
    "#has_mask_input = torch.tensor([1], dtype=torch.float)\n",
    "\n",
    "masks, scores = sam2_decoder(image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels)\n",
    "\n",
    "\n",
    "torch.onnx.export(sam2_decoder,\n",
    "                  (image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels),\n",
    "                  f\"{model_type}_decoder.onnx\",\n",
    "                  export_params=True,\n",
    "                  opset_version=16,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names = ['image_embed', 'high_res_feats_0', 'high_res_feats_1', 'point_coords', 'point_labels'],\n",
    "                  output_names = ['masks', 'iou_predictions'],\n",
    "                  #dynamic_axes = {\"point_coords\": {1: \"num_points\"},\n",
    "                                  #\"point_labels\": {1: \"num_points\"},\n",
    "                  #}\n",
    "                )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58434fe6-dd5a-408f-95aa-e627270b3129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 (64, 64) [256, 256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Datasets_and_code\\code\\realsense_python\\render_project\\final2\\python\\onnx_exporter\\sam2\\sam2\\modeling\\sam\\mask_decoder.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert image_embeddings.shape[0] == tokens.shape[0]\n",
      "D:\\Datasets_and_code\\code\\realsense_python\\render_project\\final2\\python\\onnx_exporter\\sam2\\sam2\\modeling\\sam\\mask_decoder.py:207: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  image_pe.size(0) == 1\n",
      "C:\\Users\\rescarab\\.conda\\envs\\sam_export\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:5383: UserWarning: Exporting aten::index operator of advanced indexing in opset 16 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sam2_decoder = SAM2ImageDecoder(sam2_model, multimask_output=multimask_output).cpu()\n",
    "\n",
    "embed_dim = sam2_model.sam_prompt_encoder.embed_dim\n",
    "embed_size = (sam2_model.image_size // sam2_model.backbone_stride, sam2_model.image_size // sam2_model.backbone_stride)\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "print(embed_dim, embed_size, mask_input_size)\n",
    "\n",
    "point_coords = torch.randint(low=0, high=input_size, size=(1, 20, 2), dtype=torch.float)\n",
    "point_labels = torch.randint(low=0, high=1, size=(1, 20), dtype=torch.float)\n",
    "#mask_input = torch.randn(1, 1, *mask_input_size, dtype=torch.float)\n",
    "#has_mask_input = torch.tensor([1], dtype=torch.float)\n",
    "\n",
    "masks, scores = sam2_decoder(image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels)\n",
    "\n",
    "\n",
    "torch.onnx.export(sam2_decoder,\n",
    "                  (image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels),\n",
    "                  f\"{model_type}_decoder.onnx\",\n",
    "                  export_params=True,\n",
    "                  opset_version=16,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names = ['image_embed', 'high_res_feats_0', 'high_res_feats_1', 'point_coords', 'point_labels'],\n",
    "                  output_names = ['masks', 'iou_predictions'],\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
